# -*- coding: utf-8 -*-
"""rl_project_experiment_structure

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11GtGRxpsHq41F5L97SVdxYxce3jHj9rK
"""

import ray
from ray import tune
from ray.rllib.agents.ppo import PPOTrainer
from ray.rllib.agents.ppo import PPOConfig
from popgym.envs import labyrinth_escape, labyrinth_explore

import os
import pickle
import json
import sys

import pprint
import matplotlib.pyplot as plt

"""# Configuration"""

num_of_cycles = 1 #@param
total_timesteps_per_cycle = 1000000 #@param

ray.init()

"""# Defining Environments"""

envs = ["LabyrinthEscapeEasy"]
        # , "LabyrinthEscapeMedium", "LabyrinthEscapeHard", "LabyrinthExploreEasy", "LabyrinthExploreMedium", "LabyrinthExploreHard"]

ray.tune.registry.register_env("LabyrinthEscapeEasy", lambda env_config: labyrinth_escape.LabyrinthEscapeEasy())
# ray.tune.registry.register_env("LabyrinthEscapeMedium", lambda env_config: labyrinth_escape.LabyrinthEscapeMedium())
# ray.tune.registry.register_env("LabyrinthEscapeHard", lambda env_config: labyrinth_escape.LabyrinthEscapeHard())
# ray.tune.registry.register_env("LabyrinthExploreEasy", lambda env_config: labyrinth_explore.LabyrinthExploreEasy())
# ray.tune.registry.register_env("LabyrinthExploreMedium", lambda env_config: labyrinth_explore.LabyrinthExploreMedium())
# ray.tune.registry.register_env("LabyrinthExploreHard", lambda env_config: labyrinth_explore.LabyrinthExploreHard())

"""# Defining Model"""

model = {"use_attention": True,
         "max_seq_len":30,
         # "custom_model_config":{
         "attention_num_transformer_units":2,
         "attention_dim":64,
         "attention_num_heads":20,
         # "memory_tau":50
        # }
        }

"""# Running Experiments"""

mean_reward_per_episode = {}
timesteps_done = {}
for env in envs:
    mean_reward_per_episode[env] = []
    timesteps_done[env] = []

previous_checkpoint_path = None

os.cpu_count()

total_timesteps = 0
prev_env_timesteps = 0
for cycle_count in range(num_of_cycles):
    for env in envs:
        print(f"Starting Cycle {cycle_count} Environment {env}:")
        config = {
            "env": env,
            "num_cpus": 11,  # https://discuss.ray.io/t/total-workers-number-of-gpus-1/9292/2
            "num_gpus": 4,
            "num_gpus_per_worker": 1,
            "num_cpus_per_worker": 2,
            "num_cpus_for_local_worker": 3,
            "model": model,
            "framework": "torch"
        }

        trainer = PPOTrainer(env=env, config=config)
        if previous_checkpoint_path is not None:
            trainer.restore(previous_checkpoint_path+"/"+sorted(os.listdir(previous_checkpoint_path))[-1])

        previous_checkpoint_path = f"{sys.argv[1]}/saved_checkpoints/agent_cycle_{cycle_count}_env_{env}"
        curr_env_timesteps = 0
        #variables to check whether algorithm has reached weight save points
        start_checkpoint = False
        midway_checkpoint = False
        while curr_env_timesteps < total_timesteps_per_cycle:
            result = trainer.train()

            mean_reward_per_episode[env].append(result["episode_reward_mean"])
            pprint.pprint(result)
            total_timesteps = result["timesteps_total"]
            curr_env_timesteps = total_timesteps - prev_env_timesteps
            timesteps_done[env].append(curr_env_timesteps)

            # save weights when training checkpoints are reached
            # 1 represents start checkpoint and 2 is midway checkpoint
            if not start_checkpoint and curr_env_timesteps > 0:
                print("Saving weights: Timesteps ", curr_env_timesteps)
                with open(f"{sys.argv[1]}/saved_weights/weights_cycle_{cycle_count}_env_{env}_stage_1.pkl", "wb") as f:
                    weights = trainer.get_weights()
                    pickle.dump(weights, f)
                start_checkpoint = True

            if not midway_checkpoint and curr_env_timesteps >= total_timesteps_per_cycle / 2:
                print("Saving weights: Timesteps ", curr_env_timesteps)
                with open(f"{sys.argv[1]}/saved_weights/weights_cycle_{cycle_count}_env_{env}_stage_2.pkl", "wb") as f:
                    weights = trainer.get_weights()
                    pickle.dump(weights, f)
                midway_checkpoint = True

        trainer.save(previous_checkpoint_path)

        prev_env_timesteps = total_timesteps

        print("\n\n")

ray.shutdown()

with open(f"{sys.argv[1]}/mean_reward_per_episode.json", "w") as f:
    json.dump(mean_reward_per_episode, f, indent=4)
